# Install necessary packages
!pip install torch torchvision opencv-python-headless pytesseract

# Install tesseract-ocr
!apt-get update
!apt-get install -y tesseract-ocr

# Install OpenCV for GUI functions
!apt-get install -y libgl1-mesa-glx

# Clone the YOLOv7 repository
!git clone https://github.com/WongKinYiu/yolov7.git
%cd yolov7
import torch
import cv2
import pytesseract
import numpy as np

# Load YOLOv7 model onto GPU
model = torch.hub.load('WongKinYiu/yolov7', 'yolov7', pretrained=True).cuda()
# Read the input image
image_path = "/content/frame.jpg"
image = cv2.imread(image_path)

# Convert image to RGB as YOLO model expects RGB
rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Perform YOLOv7 inference on the image
results = model(rgb_image)
detections = results.pandas().xyxy[0]  # Bounding boxes as a pandas dataframe

# Function to filter potential license plates based on typical vehicle bounding boxes
def is_vehicle(class_id):
    # COCO dataset vehicle classes (car, bus, truck)
    return class_id in [2, 3, 5, 7]

# Prepare video writer for output video
output_video_path = "/content/output_video_moving_cars.avi"
frame_height, frame_width, _ = image.shape
fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter(output_video_path, fourcc, 15, (frame_width, frame_height))  # 15 FPS for smooth movement

# Create motion effect by shifting the image
num_frames = 100  # Number of frames in the video
shift_amount = 10  # Amount by which to move the car each frame

for frame_num in range(num_frames):
    # Shift the image horizontally to simulate movement
    shifted_image = np.roll(image, shift=shift_amount, axis=1)

    # Perform YOLOv7 inference on the shifted image
    rgb_shifted_image = cv2.cvtColor(shifted_image, cv2.COLOR_BGR2RGB)
    results = model(rgb_shifted_image)
    detections = results.pandas().xyxy[0]  # Bounding boxes as a pandas dataframe

    # Iterate through detections and process vehicles
    for _, row in detections.iterrows():
        class_id = int(row['class'])
        if is_vehicle(class_id):
            # Extract bounding box coordinates
            xmin, ymin, xmax, ymax = map(int, [row['xmin'], row['ymin'], row['xmax'], row['ymax']])

            # Crop the license plate region within the vehicle bounding box
            vehicle_frame = shifted_image[ymin:ymax, xmin:xmax]
            plate_y1 = int(0.6 * (ymax - ymin))  # Lower part of vehicle
            plate_x1, plate_x2 = int(0.3 * (xmax - xmin)), int(0.7 * (xmax - xmin))
            plate_frame = vehicle_frame[plate_y1:, plate_x1:plate_x2]

            # OCR to extract license plate text
            plate_text = pytesseract.image_to_string(plate_frame, config='--psm 7')

            # Draw bounding box around the license plate
            cv2.rectangle(shifted_image, (xmin + plate_x1, ymin + plate_y1), (xmin + plate_x2, ymax), (0, 255, 0), 2)
            cv2.putText(shifted_image, plate_text.strip(), (xmin + plate_x2 + 10, ymin + plate_y1), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

    # Write the processed frame to the video
    out.write(shifted_image)

# Release the video writer and display success message
out.release()
cv2.destroyAllWindows()

print("License plate detection with moving cars completed. Video saved at", output_video_path)
from IPython.display import Video

# Display the output video in Colab
Video("/content/output_video_moving_cars.avi")







